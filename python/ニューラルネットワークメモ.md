## 2025/05/16
## ニューラルネットワークメモ
## 用語集
## ワンホットエンコーディングとは？
#### 数字で表された目的変数（ここでは0〜9の数字）を、それぞれの数字だけが「1」で、他は全部「0」の配列に変換すること。

## ソフトマックス関数
#### 出力を「確率」に変換する関数
#### たとえば、ある画像に対して10個のスコアが[2.0, 1.0, 0.1, ... , -1.0]だった場合、このままだと 「どれが答えか」分からない。ここで softmax を通すと[0.82, 0.09, 0.03, ..., 0.00]となる。
#### この数字は左から順に0~9どの数字であるかを確率で表すものになる。上記の内容だと、0という数字の場合は0.82（82%）と読め、この数字は０である可能性が高いという判別ができるようになる。→ 一番高い確率のラベルが「この画像の予測」

## バイアス（bias）とは？
#### ニューラルネットワークで「入力に重みをかけて合計したあと」に「バイアスを足す」のが基本の流れ
#### 出力=活性化関数(𝑤1𝑥1+𝑤2𝑥2+⋯+𝑤𝑛𝑥𝑛+𝑏) ここで b が バイアス
#### 何のために必要？簡単に言うと「すべての重み付き和が0だったとしても、出力に変化を与える」ため

### ニューラルネットワークのモデリング 学習メモ:

## 出力層の追加
#### 課題：「10」件の出力を返す、活性化関数がソフトマックス関数の出力層を追加し、サマリーを出力して下さい。
#### 処理：
  num_classes = 10  # 出力層をここでは10個に設定（0~9の数字認識という目的に対応するために）

  model.add(Dense(num_classes, activation='softmax'))
  
  model.summary()

#### 処理結果：
Model: "sequential_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense_2 (Dense)                 │ (None, 32)             │        25,120 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 10)             │           330 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 25,450 (99.41 KB)
 Trainable params: 25,450 (99.41 KB)
 Non-trainable params: 0 (0.00 B)


| Layer | 説明 | Output Shape（ノード数） | Param（パラメータ数） | 利用目的 |
|--|--|--|--|--|
|dense2| 中間層の結果| 32 | 25,120|特徴を抽出する中間層（Sigmoid）	|
|dense3| 出力層の結果| 10 | 330| 数字を分類する出力層（Softmax）|


#### パラメータ数の計算式

* Dense2 入力が784次元（28×28の画像）で、出力が32ノード

|計算式|
|--|
|784（入力） × 32（出力） + 32（バイアス） = 25,088 + 32 = 25,120 |



* Dense3 入力が32ノード（前の層）で、出力が10クラス

|計算式|
|--|
|32（入力） × 10（出力） + 10（バイアス） = 320 + 10 = 330 |


## 1epoch
#### 訓練データを1回にすべて使って学習すること。ここでは、epochs=4 なので、全データを4回繰り返して学習している。

#### 実行結果の読み解き方：
Epoch 1/4
1870/1875 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9057 - loss: 0.3209

Epoch 2/4
1865/1875 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9250 - loss: 0.2566

Epoch 3/4
1863/1875 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9366 - loss: 0.2158

Epoch 4/4
1858/1875 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9435 - loss: 0.1943

| 項目 | 意味（例）|
| ------------------ | -------------------------- |
| Epoch 1/4| 1回目の学習（全体で4回やる予定）|
| 1875/1875| 1回の学習の中で、小分けにされた「バッチ数」全て完了 |
| 5s| 1エポックにかかった時間（5秒）|
| 3ms/step| 1バッチあたり3ミリ秒かかった  |
| accuracy: 0.8015| このエポックの正解率（約80%）|
| loss: 0.9895| 損失（間違い具合の指標） |

#### 上記の結果からわかること：4回目のトライで、損失=0.47まで少なくなっている（初回は0.98）なので、これは間違いが少なくなって、精度が上がっていることを読み取れる。